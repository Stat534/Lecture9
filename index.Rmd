---
title: "Lecture 9: Point Level Models - Model Fitting, cont.."
output:
  revealjs::revealjs_presentation:
    theme: night
    center: true
    transition: none
    incremental: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(message = FALSE)
library(ggplot2)
library(dplyr)
library(mnormt)
library(gstat)
```

# Class Intro

## Intro Questions 
- Describe the process for finding the BLUP in this situation:

```{r}
set.seed(01312019)
krige.dat <- data.frame(x = c(1,3,5,7), y = c(log(1),log(3),log(5),log(7))+rnorm(4,0,.01)) 
krige.dat %>% ggplot(aes(x=x, y=y)) + geom_point() + xlim(1,9) + ylim(0,3) + ylab('Response') + xlab('x coordinate') + ggtitle('1D kriging illustration') 
```

## Intro Questions 

- For Today:
    - More Model Fitting

# Model Fitting

## BLUP
- It turns out that the solution for the vector $\boldsymbol{l}$ is
$$\boldsymbol{l} = \Gamma^{-1} \left( \boldsymbol{\gamma_0} + \frac{(1 - \boldsymbol{1}^T \Gamma^{-1} \boldsymbol{\gamma_0})}{\boldsymbol{1}^T \Gamma^{-1} \boldsymbol{1}}\boldsymbol{1}\right),$$
where $\Gamma$ is an $n \times n$ matrix with entries $\Gamma_{ij} = \gamma_{ij}$ and $\boldsymbol{\gamma_0}$ is the vector of $\gamma_{0i}$ values.
- Then the Best Linear Unbiased Predictor is $\boldsymbol{l}^T\boldsymbol{Y}$
- This BLUP also requires an estimate of $\gamma(\boldsymbol{h})$


## Kriging Solution

```{r, eval = F, echo = T}
# Create Gamma Matrix
x <- krige.dat$x
y <- krige.dat$y
D <- dist(x, upper=T, diag=T) %>% as.matrix()
Gamma = 1 - exp(-D/3)

# Create gamma_0 for both s1* and s2*
d1 <- sqrt((4 - x)^2) 
d2 <- sqrt((6 - x)^2) 
gamma.01 <- 1 - exp(-d1/3)
gamma.02 <- 1 - exp(-d2/3)
```

## Kriging Solution

```{r, eval = F, echo = T}
# Compute l

l.1 <- solve(Gamma) %*% 
  (gamma.01 + c(1 - rep(1,4) %*% solve(Gamma) %*% gamma.01) / 
     c(rep(1,4) %*% solve(Gamma) %*% rep(1,4)))
y1.pred <- t(l.1) %*% y

l.2 <- solve(Gamma) %*% 
  (gamma.02 + c(1 - rep(1,4) %*% solve(Gamma) %*% gamma.02) / 
     c(rep(1,4) %*% solve(Gamma) %*% rep(1,4)))
y2.pred <- t(l.2) %*% y

krige.dat %>% ggplot(aes(x=x, y=y)) + geom_point() + 
  xlim(1,9) + ylim(0,3) + ylab('Response') + xlab('x coordinate') + 
  ggtitle('1D kriging illustration with BLUPs') + 
  annotate('text',x=4,y=y1.pred, label = "s1") + 
  annotate('text',x=6,y=y2.pred, label = "s2")
```

## Kriging Solution

```{r}
# Create Gamma Matrix
x <- krige.dat$x
y <- krige.dat$y
D <- dist(x, upper=T, diag=T) %>% as.matrix()
Gamma = 1 - exp(-D/3)

# Create gamma_0 for both s1* and s2*
d1 <- sqrt((4 - x)^2) 
d2 <- sqrt((6 - x)^2) 
gamma.01 <- 1 - exp(-d1/3)
gamma.02 <- 1 - exp(-d2/3)

# Compute l

l.1 <- solve(Gamma) %*% 
  (gamma.01 + c(1 - rep(1,4) %*% solve(Gamma) %*% gamma.01) / c(rep(1,4) %*% solve(Gamma) %*% rep(1,4)))
y1.pred <- t(l.1) %*% y

l.2 <- solve(Gamma) %*% 
  (gamma.02 + c(1 - rep(1,4) %*% solve(Gamma) %*% gamma.02) / c(rep(1,4) %*% solve(Gamma) %*% rep(1,4)))
y2.pred <- t(l.2) %*% y

krige.dat %>% ggplot(aes(x=x, y=y)) + geom_point() + xlim(1,9) + ylim(0,3) + ylab('Response') + xlab('x coordinate') + ggtitle('1D kriging illustration with BLUPs') + annotate('text',x=4,y=y1.pred, label = "s1") + annotate('text',x=6,y=y2.pred, label = "s2")

```


# Kriging with Gaussian Processes

## A Gaussian Process

>- The BLUP does not contain a distributional assumptions, but rather comes from an optimization framework.
>- Now assume that $$\boldsymbol{Y} = \mu \boldsymbol{1} + \boldsymbol{\epsilon}, \; \; \text{ where } \boldsymbol{\epsilon} \sim N(\boldsymbol{0}, \Sigma)$$
>- With no nugget, let $\Sigma = \sigma^2 H(\phi)$, where $(H(\phi))_{ij} = \rho(\phi; d_{ij})$, where $d_{ij}$ is the distance between $\boldsymbol{s}_{i}$ and $\boldsymbol{s}_j$.
>- A nugget can be included by modifying $\Sigma$ to be $\Sigma = \sigma^2 H(\phi) + \tau^2 I$

## Minimizing Mean-Square Prediction Error

- __Goal:__ find $h(\boldsymbol{y})$ that minimizes
$$E[(\boldsymbol{Y}(\boldsymbol{s_0}) - h(\boldsymbol{y}))^2 | \boldsymbol{y}]$$
- $$E[(\boldsymbol{Y}(\boldsymbol{s_0}) - h(\boldsymbol{y}))^2 | \boldsymbol{y}]= E[(\boldsymbol{Y}(\boldsymbol{s_0}) - h(\boldsymbol{y}) \pm E[(\boldsymbol{Y}(\boldsymbol{s_0}|\boldsymbol{y})])^2 | \boldsymbol{y}]$$
- $$=E \{(\boldsymbol{Y}(\boldsymbol{s_0}) - E[(\boldsymbol{Y}(\boldsymbol{s_0})|\boldsymbol{y}])^2|\boldsymbol{y}  \} + \{ E[(\boldsymbol{Y}(\boldsymbol{s_0})|y] - h(\boldsymbol{y}) \}^2$$

## Minimizing Mean-Square Prediction Error: Part 2
- As $\{ E[(\boldsymbol{Y}(\boldsymbol{s_0})|y] - h(\boldsymbol{y}) \}^2 \geq 0$ 

- we have
$$E[(\boldsymbol{Y}(\boldsymbol{s_0}) - h(\boldsymbol{y}))^2 | \boldsymbol{y}] \geq E \{(\boldsymbol{Y}(\boldsymbol{s_0}) - E[(\boldsymbol{Y}(\boldsymbol{s_0})|\boldsymbol{y}])^2|\boldsymbol{y}  \}$$
- Hence to minimize $E[(\boldsymbol{Y}(\boldsymbol{s_0}) - h(\boldsymbol{y}))^2 | \boldsymbol{y}]$, we set ...

- $h(\boldsymbol{y}) = E[(\boldsymbol{Y}(\boldsymbol{s_0})|\boldsymbol{y}]$

- Hence, $h(\boldsymbol{y})$ that minimizes the error is the conditional expectation of $\boldsymbol{Y}(\boldsymbol{s_0})$

- Note this is also the *posterior mean* of $\boldsymbol{Y}(\boldsymbol{s_0})$

## Multivariate Normal Theory
- For consider partioning a multivariate normal distribution into two parts
$$\begin{pmatrix}
\boldsymbol{Y_1}\\
\boldsymbol{Y_2}
\end{pmatrix} = 
N \left( \begin{pmatrix}
\boldsymbol{\mu_1}\\
\boldsymbol{\mu_2}
\end{pmatrix}, \begin{pmatrix}
\Omega_{11} \;\;\Omega_{12}\\
\Omega_{21} \;\;\Omega_{22}\end{pmatrix}\right),$$
where $\Omega_{12} = \Omega_{21}^T$

## Conditional Multivariate Normal Theory
- The conditional distribution, $p(\boldsymbol{Y_1}|
\boldsymbol{Y_2})$ is normal with:

- $E[\boldsymbol{Y_1}|
\boldsymbol{Y_2}] = \boldsymbol{\mu_1} + \Omega_{12} \Omega_{22}^{-1} (\boldsymbol{Y_2} - \mu_2)$

- $Var[\boldsymbol{Y_1}|
\boldsymbol{Y_2}] = \Omega_{11} - \Omega_{12} \Omega_{22}^{-1} \Omega_{21}$

- Thus with $\boldsymbol{Y_1} = Y(\boldsymbol{s_0})$ and $\boldsymbol{Y_2} = \boldsymbol{y}$ 
$$\Omega_{11} = \sigma^2 + \tau^2, \; \; \; \Omega_{12} = (\sigma^2 p(\phi;d_{01})), \dots, p(\phi;d_{0n})))$$
$$\Omega_{22} = \sigma^2 H(\phi) + \tau^2 I$$



# Gaussian Process Exercise:

## Overview

- Similar to the previous exercise, we will simulate data from a 1D process and make predictions at unobserved locations - which is the interval [0,10].
- In this situation, please plot the mean of the distribution as well as some uncertainty metric.
- You do not need to estimate $\sigma^2$, $\tau^2$, and $\phi$ but can use the known values in the R code.

## Data Overview

```{r}
set.seed(02012019)
num.pts <- 6
sigma.sq <- 1
tau.sq <- 0
phi <- .75
x2 = runif(num.pts, max = 10)
mu2 <- rep(0, num.pts)
d2 <- dist(x2, upper=T, diag = T) %>% as.matrix()
Omega22 <- sigma.sq * exp(-d2 * phi)
y2 = rmnorm(1, mu2, Omega22)
GP.dat <- data.frame(x2=x2, y2 = y2)

GP.dat %>% ggplot(aes(x=x2, y=y2)) + geom_point() + xlim(0,10)  + ylab('Response') + xlab('x coordinate') + ggtitle('1D GP example')
```


## Conditional Expectation

```{r}
num.grid <- 1000
x1 <- seq(0,10, length.out = num.grid)
d11 <- dist(x1, upper=T, diag = T) %>% as.matrix()
Omega11 <- sigma.sq * exp(-d11 * phi)

d.big <- dist(c(x1,x2), upper=T, diag = T) %>% as.matrix()
d12 <- d.big[(1:num.grid),((num.grid+1):(num.pts + num.grid))]
Omega12 <- sigma.sq * exp(-d12 * phi)

cond.exp <- rep(0, num.grid) + Omega12 %*% solve(Omega22) %*% (y2 - mu2)

cond.cov <- Omega11 - Omega12 %*% solve(Omega22) %*% t(Omega12)

exp.df <- data.frame(x = x1, y = cond.exp, upper = cond.exp + 1.96 * sqrt(diag(cond.cov)), lower = cond.exp - 1.96 * sqrt(diag(cond.cov)))

GP.dat %>% ggplot(aes(x=x2, y=y2)) + geom_point() + xlim(0,10)  + ylab('Response') + xlab('x coordinate') + ggtitle('1D GP example with conditional mean') + geom_line(data = exp.df, aes(x=x, y=y), linetype = 2) 
```

## Conditional Expectation and Intervals
```{r}
GP.dat %>% ggplot(aes(x=x2, y=y2)) + geom_point() + xlim(0,10)  + ylab('Response') + xlab('x coordinate') + ggtitle('1D GP example with conditional mean and intervals') + geom_line(data = exp.df, aes(x=x, y=y), linetype = 2) +geom_line(data = exp.df, aes(x=x, y=upper), linetype = 3, color='red') + geom_line(data = exp.df, aes(x=x, y=lower), linetype = 3, color='red') 
```

## Follow up Questions

- Consider the conditional expectation, $$E[\boldsymbol{Y_1}|
\boldsymbol{Y_2}] = \boldsymbol{\mu_1} + \Omega_{12} \Omega_{22}^{-1} (\boldsymbol{Y_2} - \mu_2)$$ How is this resultant expectation impacted by $\Omega_{12} \Omega_{22}^{-1}$? How is this resultant expectation impacted by $(\boldsymbol{Y_2} - \mu_2)$? 

- Similarly, how does the conditional variance $$Var[\boldsymbol{Y_1}|
\boldsymbol{Y_2}] = \Omega_{11} - \Omega_{12} \Omega_{22}^{-1} \Omega_{21}$$ change as a function of $\Omega_{12} \Omega_{22}^{-1} \Omega_{21}$?

- If we add a nugget to the previous example, how do the predictions change?
- How does the scenario change if $\mu_1$ and $\mu_2$ are not zero, but rather say $\mu_1 = X_1 \beta$ and $\mu_2 = X_2 \beta$?


## 2D Kriging

Discuss how the spatial range could be extended to $\mathcal{R}^2$ rather than $\mathcal{R}^1$. What changes in this situation?


## Functions in R for 2D kriging
- The `krige` function in `gstat` contains a  function for kriging ; however, this requires a known variogram.

```{r, message=F, }
library(sp)
data(meuse)
coordinates(meuse) = ~x +y
data(meuse.grid)
gridded(meuse.grid) = ~x + y
m <- vgm(.59, "Sph", 874, .04)
# ordinary kriging:
x <- krige(log(zinc)~1, meuse, meuse.grid, model = m)
spplot(x["var1.pred"], main = "ordinary kriging predictions")
```




## Universal Kriging
- When covariate information is available for inclusion in the analysis, this is often referred to as *universal kriging*
- Now we have $$\boldsymbol{Y} = X \boldsymbol{\beta} + \boldsymbol{\epsilon}, \; \; \text{ where } \boldsymbol{\epsilon} \sim N(\boldsymbol{0}, \Sigma)$$
- The conditional distributions are very similar to what we have derived above, watch for HW question.

- In each case, kriging or universal kriging, it is still necessary to estimate the following parameters: $\sigma^2,$ $\tau^2$, $\phi$, and $\mu$ or $\beta$.
- This can be done with least-squares methods or in a Bayesian framework.

## About those other parameters

We still need

- to choose an appropriate covariance function (or semivariogram)
- and estimate parameters in that function
